{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CoNLL 2002 for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_conll02_rnn():\n",
    "    \n",
    "    # import data \n",
    "    tagged_sents = conll2002.iob_sents()\n",
    "    \n",
    "    # stemming, lowercasing\n",
    "    porter = PorterStemmer()\n",
    "    norm_sents = [[(porter.stem(w).lower(),pos,ne) for w,pos,ne in sent] for sent in tagged_sents if len(sent)>1]\n",
    "        # len=1 sents cause problem in rnn's batch training.\n",
    "    \n",
    "    # create look-up dicts\n",
    "    word_vocab = list({w for norm_sent in norm_sents for w,_,_ in norm_sent})\n",
    "    label_vocab = list({ne for norm_sent in norm_sents for _,_,ne in norm_sent}) \n",
    "    words2idx = defaultdict(int, {w:i for i,w in enumerate(word_vocab)})\n",
    "    labels2idx = defaultdict(int, {l:i for i,l in enumerate(label_vocab)})\n",
    "    dicts = {'words2idx':words2idx, 'labels2idx':labels2idx}\n",
    "    \n",
    "    # convert [(w,pos,ne),...] format to [w,...],[ne,...]\n",
    "    def sent_convert_rnn(norm_sent):\n",
    "        # norm_sent: a list of (w,pos,ne) triples.\n",
    "        # returns: encoded words & labels.\n",
    "        words = np.asarray([words2idx[w] for w,_,_ in norm_sent],dtype='int32')\n",
    "        labels = np.asarray([labels2idx[ne] for _,_,ne in norm_sent],dtype='int32')\n",
    "        return words, labels   \n",
    "    X, Y = [], []\n",
    "    for norm_sent in norm_sents:\n",
    "        words, labels = sent_convert_rnn(norm_sent)\n",
    "        X.append(words)\n",
    "        Y.append(labels) \n",
    "    \n",
    "    # train-valid-test split\n",
    "    def train_valid_test_split(X,Y):\n",
    "        n = len(X)\n",
    "        cutoff1, cutoff2 = int(n*.7), int(n*.85)\n",
    "        return (X[:cutoff1],Y[:cutoff1]), \\\n",
    "               (X[cutoff1:cutoff2],Y[cutoff1:cutoff2]), \\\n",
    "               (X[cutoff2:],Y[cutoff2:])  \n",
    "    train, valid, test = train_valid_test_split(X,Y)\n",
    "    \n",
    "    return {'train':train, 'valid':valid, 'test':test, 'dicts':dicts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.42 s, sys: 76.1 ms, total: 9.5 s\n",
      "Wall time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = load_conll02_rnn()\n",
    "model_dir = \"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/LUIS_PLUS/MODELS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip, cPickle, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Activation, TimeDistributed\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NER:\n",
    "    \n",
    "    def __init__(self, data, model_dir, mode='simple_rnn'):\n",
    "        '''\n",
    "        # data: a dictionary which contains ...\n",
    "            {'train':train, 'valid':valid, 'test':test, 'dicts':dicts}\n",
    "            each value in the dictionary is a 2-tuple ...\n",
    "            (encoded_sentences, encoded_labels)\n",
    "            dicts includes ...\n",
    "            'words2idx', 'labels2idx'\n",
    "        # mode: 'simple_rnn', 'lstm', 'gru'.\n",
    "        '''\n",
    "        if mode not in ['simple_rnn','lstm','gru']:\n",
    "            print \"MODE ERROR: only 'simple_rnn','lstm','gru'. \\n\"\n",
    "            return\n",
    "        self.mode = mode\n",
    "        self.model_dir = model_dir # NB: this is for temporary model saving pathing.\n",
    "        \n",
    "        transform = {'x': lambda x: np.asarray([x]), \n",
    "                     'y': lambda y: to_categorical(np.asarray(y)[:,np.newaxis],\n",
    "                                                   len(data['dicts']['labels2idx']))[np.newaxis,:,:]}\n",
    "            # len(data['dicts']['labels2idx']): number of labels.\n",
    "        try:\n",
    "            data_transformed = [ (map(transform['x'],x), map(transform['y'],y)) \n",
    "                                 for x,y in [data['train'], data['valid'], data['test']] ]\n",
    "            self.X_train, self.Y_train = data_transformed[0]\n",
    "            self.X_valid, self.Y_valid = data_transformed[1]\n",
    "            self.X_test, self.Y_test = data_transformed[2]\n",
    "            self.dicts = data['dicts']\n",
    "        except:\n",
    "            print \"DATA FORMAT ERROR: \\n\", \\\n",
    "                  \"data = {'train':train, 'valid':valid, 'test':test, 'dicts':dicts} \\n\", \\\n",
    "                  \"value = (encoded_sentences, encoded_labels) \\n\"\n",
    "    \n",
    "    def __shuffle(self, X, Y, seed):\n",
    "        '''\n",
    "        # X, Y: data and corresponding labels.\n",
    "        # seed: ensure the same after-shuffle order for X and Y.\n",
    "        '''\n",
    "        random.seed(seed)\n",
    "        random.shuffle(X)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(Y)\n",
    "    \n",
    "    def __get_mean_evaluation(self, X, Y, model):\n",
    "        '''\n",
    "        # X, Y: data and corresponding labels.\n",
    "        # return: average loss and accuracy on X and Y\n",
    "        '''\n",
    "        evaluation_size = len(X)\n",
    "        losses, accuracies = [], []\n",
    "        for i in xrange(evaluation_size):\n",
    "            loss,accuracy = model.evaluate(X[i],Y[i],verbose=0)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "        return (np.mean(losses),np.mean(accuracies))\n",
    "\n",
    "    def __save_best_model(self):\n",
    "        '''\n",
    "        # save current model as the best when called.\n",
    "        '''\n",
    "        try:\n",
    "            os.remove(self.model_dir+'best_model.p')\n",
    "            os.remove(self.model_dir+'best_weights.h5')\n",
    "        except OSError:\n",
    "            pass\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.model_dir+'best_model.p','wb') as f:\n",
    "            cPickle.dump(model_json, f)\n",
    "        self.model.save_weights(self.model_dir+'best_weights.h5')\n",
    "        print \"New Best Model Saved!\"\n",
    "    \n",
    "    def train(self, validation=False, validation_freq=1, verbose=0, verbose_freq=100,\n",
    "                    lr=.1, nhidden=100, emb_dim=100, nepochs=1,\n",
    "                    regularize=False, reg_method='l2',lmd=.1):\n",
    "        '''\n",
    "        # validation, validation_freq: \n",
    "            if true, run validation at validation_freq epoch (1 by default).\n",
    "        # verbose, verbose_freq: \n",
    "            0: only print out simple messages (e.g. '... building models').\n",
    "            1: print out validation too.\n",
    "            print training progress after training every verbose_freq sentences.\n",
    "        # lr: learning rate.\n",
    "        # nhidden: number of hidden neurons.\n",
    "        # emb_dim: dimension of word embeddings.\n",
    "        # nepochs: number of epochs.\n",
    "        # regularize: regularize or not.\n",
    "        # reg_method: Lasso or L2 regularization.\n",
    "        # lmd: regularization hyperparam.\n",
    "        # return: end training when max patience is reached.\n",
    "        '''\n",
    "        print \"... configuring model\"\n",
    "        vocsize = len(self.dicts['words2idx'])\n",
    "        nlabels = len(self.dicts['labels2idx'])\n",
    "        nsents = len(self.X_train)\n",
    "        self.dicts['idx2labels'] = {i:l for l,i in self.dicts['labels2idx'].iteritems()}\n",
    "        self.dicts['idx2words'] = {i:w for w,i in self.dicts['words2idx'].iteritems()}\n",
    "            \n",
    "        print \"... building model\" \n",
    "        if regularize:\n",
    "            regularizer = l2(lmd) if reg_method=='l2' else l1(lmd)\n",
    "        else:\n",
    "            regularizer = None\n",
    "        model_types = {'simple_rnn':SimpleRNN, 'lstm':LSTM, 'gru':GRU}\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim=vocsize, output_dim=emb_dim))\n",
    "        self.model.add(model_types[self.mode](output_dim=nhidden, activation='sigmoid', \n",
    "                       return_sequences=True, W_regularizer=regularizer))\n",
    "        self.model.add(TimeDistributed(Dense(output_dim=nlabels)))\n",
    "        self.model.add(Activation('softmax'))\n",
    "        sgd = SGD(lr=lr, momentum=.0, decay=.0, nesterov=False)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        \n",
    "        self.best_model = self.model # initial model, updated when new best models are saved.\n",
    "        \n",
    "        print \"... training model\"\n",
    "        patience = 10000\n",
    "        patience_increase_ratio = 2\n",
    "        improvement_threshold = .995\n",
    "        best_iter = 0\n",
    "        best_loss = np.inf\n",
    "        best_accuracy = 0.\n",
    "        done_looping = False # stop training when patience are broken.\n",
    "        num_iter = 0\n",
    "        for e in xrange(nepochs):\n",
    "            if done_looping: break\n",
    "            e += 1\n",
    "            if verbose: print \"... Epoch: %d\" % e\n",
    "            self.__shuffle(self.X_train,self.Y_train,seed=0)\n",
    "            for i in xrange(nsents):\n",
    "                num_iter += 1\n",
    "                if verbose and i!=0 and i%verbose_freq==0:\n",
    "                    print \"    ... trained %d sentences\" % i\n",
    "                if self.X_train[i].shape[1]==1: continue\n",
    "                self.model.train_on_batch(self.X_train[i],self.Y_train[i])\n",
    "        \n",
    "                if validation and i%validation_freq==0:\n",
    "                    sample_idxs = random.sample(range(len(self.X_valid)),100)\n",
    "                    X_valid_samples = [self.X_valid[k] for k in sample_idxs]\n",
    "                    Y_valid_samples = [self.Y_valid[k] for k in sample_idxs]\n",
    "                    avg_loss, avg_accuracy = self.__get_mean_evaluation(X_valid_samples, \n",
    "                                                                        Y_valid_samples,\n",
    "                                                                        self.model)\n",
    "                    if avg_loss < best_loss*improvement_threshold and \\\n",
    "                       avg_accuracy > best_accuracy:\n",
    "                        self.__save_best_model()\n",
    "                        patience = max(patience, num_iter*patience_increase_ratio)\n",
    "                        best_loss = avg_loss\n",
    "                        best_accuracy = avg_accuracy\n",
    "                        best_iter = num_iter\n",
    "                        print \"Validation: Loss = %.6f | Accuracy = %.6f\" % (avg_loss, avg_accuracy)                        \n",
    "                    if patience <= num_iter:\n",
    "                        done_looping = True\n",
    "                        break\n",
    "\n",
    "        mean_test_loss, mean_test_accuracy = self.__get_mean_evaluation(self.X_test,\n",
    "                                                                        self.Y_test,\n",
    "                                                                        self.best_model)\n",
    "        print \"TRAINING COMPLETE (at iteration %d)!\" % num_iter\n",
    "        print \"Test on Best Model: Loss = %.6f | Accuracy = %.6f\" % (mean_test_loss,mean_test_accuracy)\n",
    "                \n",
    "    def predict(self, x): \n",
    "        '''\n",
    "        # x: sentences in string (e.g. \"i'd like to book a flight from london to paris\")\n",
    "        # return: a list of predicted labels associated with the words in the sentence x.\n",
    "        '''\n",
    "        x_filtered = filter(lambda w:w in self.dicts['words2idx'].keys(), x.split())\n",
    "        x_encoded = np.asarray([map(self.dicts['words2idx'].get,x_filtered)])\n",
    "        pred_encoded = map(np.argmax,self.model.predict_on_batch(x_encoded)[0])\n",
    "        return map(self.dicts['idx2labels'].get, pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ner = NER(data,model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.935752 | Accuracy = 0.821422\n",
      "    ... trained 5000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.509429 | Accuracy = 0.873451\n",
      "    ... trained 10000 sentences\n",
      "    ... trained 15000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.296714 | Accuracy = 0.921065\n",
      "    ... trained 20000 sentences\n",
      "... Epoch: 2\n",
      "    ... trained 5000 sentences\n",
      "    ... trained 10000 sentences\n",
      "TRAINING COMPLETE (at iteration 30886)!\n",
      "Test on Best Model: Loss = 0.328325 | Accuracy = 0.906164\n",
      "CPU times: user 11min 23s, sys: 1min 28s, total: 12min 51s\n",
      "Wall time: 12min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ner.train(validation=1,validation_freq=5000,verbose=1,verbose_freq=5000,\n",
    "          nhidden=100,nepochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SUPER SLOW!! \n",
    "# LAST PERFORMANCE : Validation: Loss = 0.216919 | Accuracy = 0.946363 AT EPOCH 4\n",
    "# %%time\n",
    "# model = ner.train(validation=1,validation_freq=5000,verbose=1,verbose_freq=5000,\n",
    "#           nhidden=300, emb_dim=300, nepochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(model_dir+'best_model.p') as f:\n",
    "    model_json = cPickle.load(f)\n",
    "rnn_ner = model_from_json(model_json)\n",
    "rnn_ner.load_weights(model_dir+'best_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=.1, momentum=.0, decay=.0, nesterov=False)\n",
    "rnn_ner.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, Y_test = ner.X_test, ner.Y_test\n",
    "dicts = ner.dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_labels(Y): # so we can observe predictions in tags.\n",
    "    return [map(dicts['idx2labels'].get,map(np.argmax,list(y[0])))\n",
    "            for y in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = to_labels([rnn_ner.predict(x) for x in X_test])\n",
    "Y_true = to_labels(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true_in_tags = lb.fit_transform(list(chain.from_iterable(Y_true))) # one-hot tags\n",
    "y_pred_in_tags = lb.transform(list(chain.from_iterable(Y_pred)))\n",
    "tagset = list(set(lb.classes_))\n",
    "class_indices = {cls:idx for idx,cls in enumerate(lb.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.93      0.99      0.96     61463\n",
      "      I-LOC       0.00      0.00      0.00       313\n",
      "      B-ORG       0.18      0.17      0.17       386\n",
      "      I-PER       0.68      0.19      0.29      1037\n",
      "      B-PER       0.45      0.29      0.35      1428\n",
      "     I-MISC       0.00      0.00      0.00       561\n",
      "     B-MISC       0.00      0.00      0.00       974\n",
      "      I-ORG       0.68      0.26      0.38       282\n",
      "      B-LOC       0.51      0.40      0.44      1250\n",
      "\n",
      "avg / total       0.88      0.91      0.89     67694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(\n",
    "    y_true_in_tags,\n",
    "    y_pred_in_tags,\n",
    "    labels = [class_indices[cls] for cls in tagset],\n",
    "    target_names = tagset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
