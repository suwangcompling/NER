{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "import spacy\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_info(sent): \n",
    "    # assuming parser = spacy.English()\n",
    "    # sent is a list of words\n",
    "    if type(sent)==list: sent = ' '.join(sent)\n",
    "    parsed = parser(unicode(sent))# if type(sent)==str else parser(unicode(sent))\n",
    "    pos = [token.pos_ for token in parsed]\n",
    "    ner = ['none' if token.ent_type_=='' else token.ent_type_ for token in parsed]\n",
    "    dep_rel = [token.dep_ for token in parsed]\n",
    "    dep_head = [token.head.orth_ for token in parsed]\n",
    "    return pos, ner, dep_rel, dep_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/LUIS/DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = gzip.open(path+'atis.fold0.pkl.gz','rb')\n",
    "train, valid, test, dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([554, 194, 268,  64,  62,  16,   8, 234, 481,  20,  40,  58, 234,\n",
       "       415, 205], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels2idx', 'tables2idx', 'words2idx']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts.keys() # 'tables2idx' is not used, since it's not derivable generally (atis-specific info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i2w = {i:w for w,i in dicts['words2idx'].iteritems()}\n",
    "i2l = {i:l for l,i in dicts['labels2idx'].iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'flights', 'leave', 'atlanta', 'at', 'about', 'DIGIT', 'in', 'the', 'afternoon', 'and', 'arrive', 'in', 'san', 'francisco']\n"
     ]
    }
   ],
   "source": [
    "print map(i2w.get, train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [map(i2w.get, encoded_sent) for encoded_sent in train[0]]\n",
    "X_test = [map(i2w.get, encoded_sent) for encoded_sent in test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'flights', 'leave', 'atlanta', 'at', 'about', 'DIGIT', 'in', 'the', 'afternoon', 'and', 'arrive', 'in', 'san', 'francisco']\n",
      "['i', 'would', 'like', 'to', 'find', 'a', 'flight', 'from', 'charlotte', 'to', 'las', 'vegas', 'that', 'makes', 'a', 'stop', 'in', 'st.', 'louis']\n"
     ]
    }
   ],
   "source": [
    "print X_train[0]\n",
    "print X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = [map(i2l.get, encoded_labels) for encoded_labels in train[2]]\n",
    "Y_test = [map(i2l.get, encoded_sent) for encoded_sent in test[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-depart_time.time_relative', 'B-depart_time.time', 'O', 'O', 'B-depart_time.period_of_day', 'O', 'O', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'O', 'O', 'O', 'O', 'B-stoploc.city_name', 'I-stoploc.city_name']\n"
     ]
    }
   ],
   "source": [
    "print Y_train[0]\n",
    "print Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dicts['labels2idx'].keys()) # 127 labels in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_info(sent): \n",
    "    # sent: a list of words.\n",
    "    # return: (words, pos, ner, dep_rel, dep_head).\n",
    "    pos, ner, dep_rel, dep_head = extract_info(sent)\n",
    "    return (sent, pos, ner, dep_rel, dep_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['what', 'flights', 'leave', 'atlanta', 'at', 'about', 'DIGIT', 'in', 'the', 'afternoon', 'and', 'arrive', 'in', 'san', 'francisco'], [u'ADJ', u'NOUN', u'VERB', u'ADV', u'ADP', u'ADP', u'PROPN', u'ADP', u'DET', u'NOUN', u'CONJ', u'VERB', u'ADP', u'PROPN', u'NOUN'], ['none', 'none', 'none', 'none', 'none', 'none', u'ORG', 'none', 'none', u'TIME', 'none', 'none', 'none', 'none', 'none'], [u'det', u'nsubj', u'ROOT', u'advmod', u'prep', u'nmod', u'pobj', u'prep', u'det', u'pobj', u'cc', u'conj', u'prep', u'compound', u'pobj'], [u'flights', u'leave', u'leave', u'at', u'leave', u'DIGIT', u'at', u'DIGIT', u'afternoon', u'in', u'leave', u'leave', u'arrive', u'francisco', u'in'])\n"
     ]
    }
   ],
   "source": [
    "print augment_info(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_augmented = [augment_info(sent) for sent in X_train]\n",
    "X_test_augmented = [augment_info(sent) for sent in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'flights', 'leave', 'atlanta', 'at', 'about', 'DIGIT', 'in', 'the', 'afternoon', 'and', 'arrive', 'in', 'san', 'francisco']\n",
      "[u'ADJ', u'NOUN', u'VERB', u'ADV', u'ADP', u'ADP', u'PROPN', u'ADP', u'DET', u'NOUN', u'CONJ', u'VERB', u'ADP', u'PROPN', u'NOUN']\n",
      "['none', 'none', 'none', 'none', 'none', 'none', u'ORG', 'none', 'none', u'TIME', 'none', 'none', 'none', 'none', 'none']\n",
      "[u'det', u'nsubj', u'ROOT', u'advmod', u'prep', u'nmod', u'pobj', u'prep', u'det', u'pobj', u'cc', u'conj', u'prep', u'compound', u'pobj']\n",
      "[u'flights', u'leave', u'leave', u'at', u'leave', u'DIGIT', u'at', u'DIGIT', u'afternoon', u'in', u'leave', u'leave', u'arrive', u'francisco', u'in']\n"
     ]
    }
   ],
   "source": [
    "for entry in X_train_augmented[0]:\n",
    "    print entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyparsing import StringEnd, oneOf, FollowedBy, Optional, ZeroOrMore, SkipTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = ['anti','de','dis','en','em','fore','in','im','il','ir',\n",
    "          'inter','mid','mis','non','over','pre','re','semi','sub',\n",
    "          'super','trans','un','under']\n",
    "suffix = ['able','ible','al','ial','ed','en','er','est','ful','ic',\n",
    "          'ing','ion','tion','ation','ition','ity','ty','ive','ative',\n",
    "          'itive','less','ly','ment','ness','ous','eous','ious','s',\n",
    "          'es','y','ism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Featurize:\n",
    "    \n",
    "    def __init__(self, prefix=[], suffix=[]): # lists of pfx/sfx.\n",
    "        self.prefix = prefix\n",
    "        self.suffix = suffix\n",
    "        end_of_string = StringEnd()\n",
    "        pfx_pyp_regex = oneOf(' '.join(prefix))\n",
    "        sfx_pyp_regex = oneOf(' '.join(suffix)) + FollowedBy(end_of_string)\n",
    "        self.template = (ZeroOrMore(pfx_pyp_regex)('prefix') +\n",
    "                         SkipTo(sfx_pyp_regex | end_of_string)('root') + \n",
    "                         Optional(sfx_pyp_regex)('suffix'))              \n",
    "        self.afx = lambda word: self.template.parseString(word)\n",
    "        self.feat_set = {'pfx': lambda w_idx,datum: self.afx(datum[0][w_idx]).prefix[0] \\\n",
    "                                     if self.afx(datum[0][w_idx]).prefix!='' else 'no_pfx',\n",
    "                         'sfx': lambda w_idx,datum: self.afx(datum[0][w_idx]).suffix[0] \\\n",
    "                                     if self.afx(datum[0][w_idx]).suffix!='' else 'no_sfx',\n",
    "                         'root': lambda w_idx,datum: self.afx(datum[0][w_idx]).root,\n",
    "                         'isdigit': lambda w_idx,datum: datum[0][w_idx].isdigit(),\n",
    "                         'pos': lambda w_idx,datum: datum[1][w_idx],\n",
    "                         'ner': lambda w_idx,datum: datum[2][w_idx],\n",
    "                         'dep_rel': lambda w_idx,datum: datum[3][w_idx],\n",
    "                         'dep_head': lambda w_idx,datum: datum[4][w_idx]} \n",
    "            # datum: (words, pos, ner, dep_rel, dep_head)\n",
    "        \n",
    "    def word_featurize(self, datum, i): \n",
    "        # datum: (sent, pos, ner, dep_rel, dep_head).\n",
    "        # i: index of the token processed. \n",
    "        features = {}\n",
    "        for feat in self.feat_set.keys():\n",
    "            features[feat] = str(self.feat_set[feat](i,datum))\n",
    "        features['boundary'] = 'No'\n",
    "        if i > 0:\n",
    "            for feat in self.feat_set.keys():\n",
    "                features[feat+'-1'] = str(self.feat_set[feat](i-1,datum))\n",
    "                if i > 1:\n",
    "                    features[feat+'-2'] = str(self.feat_set[feat](i-2,datum))\n",
    "        else: features['boundary'] = 'BOS'\n",
    "        if i < len(datum[0])-1:\n",
    "            for feat in self.feat_set.keys():\n",
    "                features[feat+'+1'] = str(self.feat_set[feat](i+1,datum))\n",
    "                if i < len(datum[0])-2:\n",
    "                    features[feat+'+2'] = str(self.feat_set[feat](i+2,datum))\n",
    "        else: features['boundary'] = 'EOS'\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    def sent_featurize(self, datum):\n",
    "        # datum: (sent, pos, ner, dep_rel, dep_head).   \n",
    "        return [self.word_featurize(datum, i) for i in xrange(len(datum[0]))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurizer = Featurize(prefix,suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['what', 'flights', 'leave', 'atlanta', 'at', 'about', 'DIGIT', 'in', 'the', 'afternoon', 'and', 'arrive', 'in', 'san', 'francisco'], [u'ADJ', u'NOUN', u'VERB', u'ADV', u'ADP', u'ADP', u'PROPN', u'ADP', u'DET', u'NOUN', u'CONJ', u'VERB', u'ADP', u'PROPN', u'NOUN'], ['none', 'none', 'none', 'none', 'none', 'none', u'ORG', 'none', 'none', u'TIME', 'none', 'none', 'none', 'none', 'none'], [u'det', u'nsubj', u'ROOT', u'advmod', u'prep', u'nmod', u'pobj', u'prep', u'det', u'pobj', u'cc', u'conj', u'prep', u'compound', u'pobj'], [u'flights', u'leave', u'leave', u'at', u'leave', u'DIGIT', u'at', u'DIGIT', u'afternoon', u'in', u'leave', u'leave', u'arrive', u'francisco', u'in'])\n"
     ]
    }
   ],
   "source": [
    "test_sent = X_train_augmented[0]\n",
    "print test_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 'NOUN', 'pfx+2': 'no_pfx', 'dep_head+2': 'at', 'dep_head+1': 'leave', 'isdigit+2': 'False', 'isdigit': 'False', 'dep_head': 'leave', 'isdigit+1': 'False', 'pfx+1': 'no_pfx', 'pfx-1': 'no_pfx', 'dep_head-1': 'flights', 'isdigit-1': 'False', 'dep_rel-1': 'det', 'dep_rel+2': 'advmod', 'ner+1': 'none', 'root-1': 'what', 'boundary': 'No', 'dep_rel': 'nsubj', 'root+2': 'atlanta', 'root+1': 'leave', 'ner+2': 'none', 'ner-1': 'none', 'dep_rel+1': 'ROOT', 'sfx': 's', 'sfx-1': 'no_sfx', 'sfx+1': 'no_sfx', 'pfx': 'no_pfx', 'sfx+2': 'no_sfx', 'pos-1': 'ADJ', 'ner': 'none', 'pos+2': 'ADV', 'pos+1': 'VERB', 'root': 'flight'}\n"
     ]
    }
   ],
   "source": [
    "print featurizer.word_featurize(test_sent,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 'ADJ', 'isdigit': 'False', 'dep_head+2': 'leave', 'dep_head+1': 'leave', 'pfx+2': 'no_pfx', 'isdigit+2': 'False', 'isdigit+1': 'False', 'pfx+1': 'no_pfx', 'dep_rel+2': 'ROOT', 'ner+1': 'none', 'boundary': 'BOS', 'dep_rel': 'det', 'root+2': 'leave', 'root+1': 'flight', 'ner+2': 'none', 'dep_head': 'flights', 'dep_rel+1': 'nsubj', 'sfx': 'no_sfx', 'sfx+1': 's', 'pfx': 'no_pfx', 'sfx+2': 'no_sfx', 'ner': 'none', 'pos+2': 'VERB', 'pos+1': 'NOUN', 'root': 'what'}\n"
     ]
    }
   ],
   "source": [
    "print featurizer.sent_featurize(test_sent)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = [(x_augmented,y) for x_augmented,y in zip(X_train_augmented,Y_train)]\n",
    "test_sents = [(x_augmented,y) for x_augmented,y in zip(X_test_augmented,Y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['what', 'flights', 'leave', 'atlanta', 'at', 'about', 'DIGIT', 'in', 'the', 'afternoon', 'and', 'arrive', 'in', 'san', 'francisco'], [u'ADJ', u'NOUN', u'VERB', u'ADV', u'ADP', u'ADP', u'PROPN', u'ADP', u'DET', u'NOUN', u'CONJ', u'VERB', u'ADP', u'PROPN', u'NOUN'], ['none', 'none', 'none', 'none', 'none', 'none', u'ORG', 'none', 'none', u'TIME', 'none', 'none', 'none', 'none', 'none'], [u'det', u'nsubj', u'ROOT', u'advmod', u'prep', u'nmod', u'pobj', u'prep', u'det', u'pobj', u'cc', u'conj', u'prep', u'compound', u'pobj'], [u'flights', u'leave', u'leave', u'at', u'leave', u'DIGIT', u'at', u'DIGIT', u'afternoon', u'in', u'leave', u'leave', u'arrive', u'francisco', u'in'])\n",
      "\n",
      "['O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-depart_time.time_relative', 'B-depart_time.time', 'O', 'O', 'B-depart_time.period_of_day', 'O', 'O', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n"
     ]
    }
   ],
   "source": [
    "print train_sents[0][0]\n",
    "print \n",
    "print train_sents[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 602 ms, total: 1min 36s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [featurizer.sent_featurize(datum[0]) for datum in train_sents]\n",
    "Y_train = [datum[1] for datum in train_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 'ADJ', 'isdigit': 'False', 'dep_head+2': 'leave', 'dep_head+1': 'leave', 'pfx+2': 'no_pfx', 'isdigit+2': 'False', 'isdigit+1': 'False', 'pfx+1': 'no_pfx', 'dep_rel+2': 'ROOT', 'ner+1': 'none', 'boundary': 'BOS', 'dep_rel': 'det', 'root+2': 'leave', 'root+1': 'flight', 'ner+2': 'none', 'dep_head': 'flights', 'dep_rel+1': 'nsubj', 'sfx': 'no_sfx', 'sfx+1': 's', 'pfx': 'no_pfx', 'sfx+2': 'no_sfx', 'ner': 'none', 'pos+2': 'VERB', 'pos+1': 'NOUN', 'root': 'what'}\n",
      "\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "print X_train[0][0]\n",
    "print \n",
    "print Y_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [(w_feats,label) for X,Y in zip(X_train,Y_train) for w_feats,label in zip(X,Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pos': 'ADJ', 'isdigit': 'False', 'dep_head+2': 'leave', 'dep_head+1': 'leave', 'pfx+2': 'no_pfx', 'isdigit+2': 'False', 'isdigit+1': 'False', 'pfx+1': 'no_pfx', 'dep_rel+2': 'ROOT', 'ner+1': 'none', 'boundary': 'BOS', 'dep_rel': 'det', 'root+2': 'leave', 'root+1': 'flight', 'ner+2': 'none', 'dep_head': 'flights', 'dep_rel+1': 'nsubj', 'sfx': 'no_sfx', 'sfx+1': 's', 'pfx': 'no_pfx', 'sfx+2': 'no_sfx', 'ner': 'none', 'pos+2': 'VERB', 'pos+1': 'NOUN', 'root': 'what'}, 'O')\n",
      "\n",
      "({'pos': 'NOUN', 'pfx+2': 'no_pfx', 'dep_head+2': 'at', 'dep_head+1': 'leave', 'isdigit+2': 'False', 'isdigit': 'False', 'dep_head': 'leave', 'isdigit+1': 'False', 'pfx+1': 'no_pfx', 'pfx-1': 'no_pfx', 'dep_head-1': 'flights', 'isdigit-1': 'False', 'dep_rel-1': 'det', 'dep_rel+2': 'advmod', 'ner+1': 'none', 'root-1': 'what', 'boundary': 'No', 'dep_rel': 'nsubj', 'root+2': 'atlanta', 'root+1': 'leave', 'ner+2': 'none', 'ner-1': 'none', 'dep_rel+1': 'ROOT', 'sfx': 's', 'sfx-1': 'no_sfx', 'sfx+1': 'no_sfx', 'pfx': 'no_pfx', 'sfx+2': 'no_sfx', 'pos-1': 'ADJ', 'ner': 'none', 'pos+2': 'ADV', 'pos+1': 'VERB', 'root': 'flight'}, 'O')\n"
     ]
    }
   ],
   "source": [
    "print train[0]\n",
    "print\n",
    "print train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 144 ms, total: 19.5 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = [featurizer.sent_featurize(datum[0]) for datum in test_sents]\n",
    "Y_test = [datum[1] for datum in test_sents]\n",
    "test = [(w_feats,label) for X,Y in zip(X_test,Y_test) for w_feats,label in zip(X,Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from nltk import MaxentClassifier, classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 13s, sys: 2.67 s, total: 3min 16s\n",
      "Wall time: 12min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "me_lbfgs = MaxentClassifier.train(train, algorithm='megam', trace=3, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L-BFGS: 90.86%\n"
     ]
    }
   ],
   "source": [
    "print \"L-BFGS: %.2f%%\" % (classify.accuracy(me_lbfgs,test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
