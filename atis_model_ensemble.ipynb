{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "** Idea: We have 127 labels in the original data. To improve prediction accuracy, these are grouped into 10 \"meta-labels\", and each would have a separate RNN model at its service. To train a children-labelset (i.e. the set of labels under a meta-label) separately, we need to modify the labels in the original data: All labels in the children-labelset are mapped to new index-encodings, whereas the labels not in the children-labelset are treated as 'O' label. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT LABEL SCHEME (I.E. META-CHILDREN HIERARCHY)\n",
    "import json, gzip, pickle, os\n",
    "os.chdir('/Users/jacobsw/Desktop')\n",
    "with open('new_ojoatis.json') as json_file:\n",
    "    ojoatis = json.load(json_file)\n",
    "entities = ojoatis['entities']\n",
    "luis_utterances = ojoatis['utterances']\n",
    "# entities = \n",
    "# [{u'children': [u'arrive_time.end_time',\n",
    "#    u'arrive_time.period_mod',\n",
    "#    u'arrive_time.period_of_day',\n",
    "#    u'arrive_time.start_time',\n",
    "#    u'arrive_time.time_relative',\n",
    "#    u'arrive_time.time'],\n",
    "#   u'name': u'arrive_time'},\n",
    "#  { ... },\n",
    "#  ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_atis(path, folder=0):\n",
    "    \n",
    "    folders = {0: 'atis.fold0.pkl.gz',\n",
    "               1: 'atis.fold1.pkl.gz',\n",
    "               2: 'atis.fold2.pkl.gz',\n",
    "               3: 'atis.fold3.pkl.gz',\n",
    "               4: 'atis.fold4.pkl.gz'}\n",
    "    \n",
    "    f = gzip.open(path+folders[folder], 'rb')\n",
    "    train, valid, test, dicts = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    train = (train[0],train[2])\n",
    "    valid = (valid[0],valid[2])\n",
    "    test = (test[0],test[2])\n",
    "\n",
    "    return {'train':train, 'valid':valid, 'test':test, 'dicts':dicts}\n",
    "\n",
    "# ATIS PATH: /Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/LUIS/DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entities_normalize(entities):\n",
    "    \"\"\"\n",
    "    # entities:\n",
    "        a list of dictionaries for each\n",
    "        keys = {'name', 'children'}.    \n",
    "      TRANSFORMED EXAMPLE:\n",
    "        {u'aircraft_code': u'codes_types',\n",
    "         u'airline_code': u'codes_types',\n",
    "         u'airline_name': u'from_loc',\n",
    "         u'airport_code': u'codes_types',\n",
    "         u'airport_name': u'from_loc',\n",
    "         u'arrive_date.date_relative': u'arrive_date',\n",
    "         u'arrive_date.day_name': u'arrive_date',\n",
    "         u'arrive_date.day_number': u'arrive_date',\n",
    "         ...\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for entry in entities:\n",
    "        for child in entry['children']:\n",
    "            new_dict[child] = entry['name']\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    \n",
    "    def __init__(self, data, entities):\n",
    "        '''\n",
    "        # data: original data, with the structure\n",
    "          {'train':(X_train,Y_train),'valid':(X_valid,Y_valid),'test':(X_test,Y_test),\n",
    "            'dicts':dicts}\n",
    "        # entities:\n",
    "            a list of dictionaries for each\n",
    "            keys = {'name', 'children'}.\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.entities = entities\n",
    "        self.entities_normalized = self.__entities_normalize(entities)\n",
    "        self.meta_labels = [entity['name'] for entity in self.entities]\n",
    "        self.meta_dict = defaultdict(dict)\n",
    "        self.new_datasets = defaultdict(dict)\n",
    "        for meta_label in self.meta_labels:\n",
    "            self.meta_dict[meta_label] = self.__create_new_labels2idx(meta_label)\n",
    "            self.new_datasets[meta_label] = self.__label_transform(meta_label, self.data)\n",
    "    \n",
    "    def __entities_normalize(self, entities):\n",
    "        '''\n",
    "        Turns a list of {'name':parent,'children':[child,...]}, i.e. entities\n",
    "        into a mapping from children to parent.\n",
    "        \n",
    "        # entities:\n",
    "            a list of dictionaries for each\n",
    "            keys = {'name', 'children'}.    \n",
    "          TRANSFORMED EXAMPLE:\n",
    "            {u'aircraft_code': u'codes_types',\n",
    "             u'airline_code': u'codes_types',\n",
    "             u'airline_name': u'from_loc',\n",
    "             u'airport_code': u'codes_types',\n",
    "             u'airport_name': u'from_loc',\n",
    "             u'arrive_date.date_relative': u'arrive_date',\n",
    "             u'arrive_date.day_name': u'arrive_date',\n",
    "             u'arrive_date.day_number': u'arrive_date',\n",
    "             ...\n",
    "        @ return: transformed children-to-parent dictionary.\n",
    "        '''\n",
    "        new_dict = {}\n",
    "        for entry in entities:\n",
    "            for child in entry['children']:\n",
    "                new_dict[child] = entry['name']\n",
    "        return new_dict\n",
    "    \n",
    "    def __create_new_labels2idx(self, meta_label):\n",
    "        '''\n",
    "        Create a dictionary with keys being the labels in the original dataset,\n",
    "        and values being {'new_idx':.., 'old_idx':..} sub-dictionaries.\n",
    "        \n",
    "        # meta_label: e.g. 'arrive_time'.\n",
    "        @ return: new dictionary.\n",
    "        '''\n",
    "        idx = 0\n",
    "        new_dict = defaultdict(dict)\n",
    "        new_dict['O']['new_idx'] = idx # 'O' will always have index 0\n",
    "        new_dict['O']['old_idx'] = self.data['dicts']['labels2idx']['O']\n",
    "        idx += 1\n",
    "        for key in data['dicts']['labels2idx'].iterkeys():\n",
    "            if key!= 'O' and self.entities_normalized[key[2:]] == meta_label:\n",
    "                new_dict[key]['new_idx'] = idx\n",
    "                idx += 1\n",
    "            else: \n",
    "                new_dict[key]['new_idx'] = 0 \n",
    "            new_dict[key]['old_idx'] = data['dicts']['labels2idx'][key]\n",
    "        return new_dict\n",
    "    \n",
    "    def __label_transform(self, meta_label, data):\n",
    "        '''\n",
    "        Transform the original 127-label dataset into a k-label new dataset\n",
    "        k depends on the meta_label.\n",
    "        \n",
    "        # data: original data, with the structure\n",
    "          {'train':(X_train,Y_train),'valid':(X_valid,Y_valid),'test':(X_test,Y_test),\n",
    "            'dicts':dicts}\n",
    "        @ return: label-transformed data.\n",
    "        '''\n",
    "        X_train, X_valid, X_test = data['train'][0], data['valid'][0], data['test'][0]\n",
    "        Y_train, Y_valid, Y_test = deepcopy(data['train'][1]), deepcopy(data['valid'][1]), deepcopy(data['test'][1])\n",
    "        old_i2l = {i:l for l,i in data['dicts']['labels2idx'].iteritems()}\n",
    "        l2_o_n_idx = self.meta_dict[meta_label]\n",
    "        new_l2i = defaultdict(int)\n",
    "        new_l2i['O'] = 0\n",
    "        for label,o_n_pair in l2_o_n_idx.iteritems():\n",
    "            if o_n_pair['new_idx'] != 0:\n",
    "                new_l2i[label] = o_n_pair['new_idx']\n",
    "        for labelidx_lists in [Y_train,Y_valid,Y_test]:\n",
    "            for i,labelidx_list in enumerate(labelidx_lists):\n",
    "                for j,labelidx in enumerate(labelidx_list):\n",
    "                    label = old_i2l[labelidx]\n",
    "                    new_label = l2_o_n_idx[label]['new_idx']\n",
    "                    labelidx_lists[i][j] = new_label\n",
    "        new_dicts = {'words2idx':data['dicts']['words2idx'], 'labels2idx':new_l2i}\n",
    "        return {'train':(X_train,Y_train),'valid':(X_valid,Y_valid),'test':(X_test,Y_test),\n",
    "                'dicts':new_dicts}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_labels = [entity['name'] for entity in entities]\n",
    "# [arrive_time, depart_time_meal, return_cost, flight, from_loc, \n",
    "#  codes_types, misc_date_time, depart_date_mod_or, stop_to_loc,\n",
    "#  arrive_date ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_atis(path='/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/LUIS/DATA/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtf = DataTransform(data, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = dtf.new_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: RNN-Keras-Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip, pickle, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Activation, TimeDistributed\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NER:\n",
    "    \n",
    "    def __init__(self, data, model_dir, mode='SimpleRNN'):\n",
    "        '''\n",
    "        # data: a dictionary which contains ...\n",
    "            {'train':train, 'valid':valid, 'test':test, 'dicts':dicts}\n",
    "            each value in the dictionary is a 2-tuple ...\n",
    "            (encoded_sentences, encoded_labels)\n",
    "            dicts includes ...\n",
    "            'words2idx', 'labels2idx'\n",
    "        # mode: 'SimpleRNN', 'LSTM', 'GRU'.\n",
    "        '''\n",
    "        if mode not in ['SimpleRNN','LSTM','GRU']:\n",
    "            print \"MODE ERROR: only 'SimpleRNN', 'LSTM', 'GRU'. \\n\"\n",
    "            return\n",
    "        self.mode = mode\n",
    "        try:\n",
    "            self.model_dir = model_dir\n",
    "        except IOError:\n",
    "            print \"IOError: Check if directory is correct.\"\n",
    "        try:\n",
    "            data_transformed = [ (map(self.__transform(len(data['dicts']['labels2idx']),'x'),x),\n",
    "                                      map(self.__transform(len(data['dicts']['labels2idx']),'y'),y)) \n",
    "                                      for x,y in [data['train'], \n",
    "                                                  data['valid'], \n",
    "                                                  data['test']] ]\n",
    "            self.X_train, self.Y_train = data_transformed[0]\n",
    "            self.X_valid, self.Y_valid = data_transformed[1]\n",
    "            self.X_test, self.Y_test = data_transformed[2]\n",
    "            self.dicts = data['dicts']\n",
    "        except:\n",
    "            print \"DATA FORMAT ERROR: \\n\", \\\n",
    "                  \"data = {'train':train, 'valid':valid, 'test':test, 'dicts':dicts} \\n\", \\\n",
    "                  \"value = (encoded_sentences, encoded_labels) \\n\"\n",
    "    \n",
    "    def __transform(self, nlabels, mode='x'):\n",
    "        '''\n",
    "        # x: (sent_len, ) => (1, sent_len)\n",
    "        # y: (sent_len, ) => (1, sent_len, nlabels)\n",
    "        '''\n",
    "        if mode not in ['x','y']: \n",
    "            print \"MODE ERROR: only 'x' and 'y'. \\n\"\n",
    "            return\n",
    "        if mode=='x': return lambda x: np.asarray([x])  \n",
    "        if mode=='y': return lambda y: to_categorical(np.asarray(y)[:,np.newaxis],\n",
    "                                                  nlabels)[np.newaxis,:,:] \n",
    "    \n",
    "    def __shuffle(self, X, Y, seed):\n",
    "        '''\n",
    "        # X, Y: data and corresponding labels.\n",
    "        # seed: ensure the same after-shuffle order for X and Y.\n",
    "        '''\n",
    "        random.seed(seed)\n",
    "        random.shuffle(X)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(Y)\n",
    "    \n",
    "    def __get_mean_evaluation(self, X, Y):\n",
    "        '''\n",
    "        # X, Y: data and corresponding labels.\n",
    "        # return: average loss and accuracy on X and Y\n",
    "        '''\n",
    "        losses, accuracies = [], []\n",
    "        for i in xrange(100):\n",
    "            loss,accuracy = ner.model.evaluate(X[i],Y[i],verbose=0)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "        return (np.mean(losses),np.mean(accuracies))\n",
    "\n",
    "    def __save_best_model(self,mode):\n",
    "        try:\n",
    "            os.remove(self.model_dir+mode+'.json')\n",
    "            os.remove(self.model_dir+mode+'.h5')\n",
    "        except OSError:\n",
    "            pass\n",
    "        model_json = self.model.to_json()\n",
    "        open(self.model_dir+mode+'.json','w').write(model_json)\n",
    "        self.model.save_weights(self.model_dir+mode+'.h5')\n",
    "        print \"New %s Saved!\" % mode    \n",
    "    \n",
    "    def train(self, validation=False, validation_freq=1, verbose=0,\n",
    "                    lr=.1, nhidden=100, emb_dim=100, nepochs=1):\n",
    "        '''\n",
    "        # validation, validation_freq: \n",
    "            if true, run validation at validation_freq epoch (1 by default).\n",
    "        # verbose, verbose_freq: \n",
    "            0: only print out simple messages (e.g. '... building models').\n",
    "            1: print out validation too.\n",
    "            print training progress after training every verbose_freq sentences.\n",
    "        # lr: learning rate.\n",
    "        # nhidden: number of hidden neurons.\n",
    "        # emb_dim: dimension of word embeddings.\n",
    "        # nepochs: number of epochs.\n",
    "        # return: end training when max patience is reached.\n",
    "        '''\n",
    "        print \"... configuring model\"\n",
    "        vocsize = len(self.dicts['words2idx'])\n",
    "        nlabels = len(self.dicts['labels2idx'])\n",
    "        nsents = len(self.X_train)\n",
    "        self.dicts['idx2labels'] = {i:l for l,i in self.dicts['labels2idx'].iteritems()}\n",
    "        self.dicts['idx2words'] = {i:w for w,i in self.dicts['words2idx'].iteritems()}\n",
    "            \n",
    "        print \"... building model\"            \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim=vocsize, output_dim=emb_dim))\n",
    "        if self.mode=='LSTM':\n",
    "            self.model.add(LSTM(output_dim=nhidden, activation='sigmoid', return_sequences=True))\n",
    "        elif self.mode=='GRU':\n",
    "            self.model.add(GRU(output_dim=nhidden, activation='sigmoid', return_sequences=True))\n",
    "        else: \n",
    "            self.model.add(SimpleRNN(output_dim=nhidden, activation='sigmoid', return_sequences=True))\n",
    "        self.model.add(TimeDistributed(Dense(output_dim=nlabels)))\n",
    "        self.model.add(Activation('softmax'))\n",
    "        sgd = SGD(lr=lr, momentum=.0, decay=.0, nesterov=False)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        \n",
    "        print \"... training model\"\n",
    "        patience = 3000\n",
    "        patience_increase_ratio = 2\n",
    "        improvement_threshold = .995\n",
    "        best_iter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.best_accuracy = 0.\n",
    "        for e in xrange(nepochs):\n",
    "            e += 1\n",
    "            if verbose: print \"... Epoch: %d\" % e\n",
    "            self.__shuffle(self.X_train,self.Y_train,seed=0)\n",
    "            for i in xrange(nsents):\n",
    "                if self.X_train[i].shape[1]==1: continue\n",
    "                self.model.train_on_batch(self.X_train[i],self.Y_train[i])\n",
    "        \n",
    "                if validation and i%validation_freq==0:\n",
    "                    sample_idxs = random.sample(range(len(self.X_valid)),100)\n",
    "                    X_valid_samples = [self.X_valid[k] for k in sample_idxs]\n",
    "                    Y_valid_samples = [self.Y_valid[k] for k in sample_idxs]\n",
    "                    avg_loss, avg_accuracy = self.__get_mean_evaluation(X_valid_samples, \n",
    "                                                                        Y_valid_samples)\n",
    "                    current_iter = ((e-1)*nsents+i)\n",
    "                    if avg_loss < self.best_loss*improvement_threshold and \\\n",
    "                       avg_accuracy > self.best_accuracy:\n",
    "                        self.__save_best_model(mode='best_valid_model')\n",
    "                        patience = max(patience, current_iter*patience_increase_ratio)\n",
    "                        self.best_loss = avg_loss\n",
    "                        self.best_accuracy = avg_accuracy\n",
    "                        best_iter = current_iter\n",
    "                        sample_idxs = random.sample(range(len(self.X_test)),100)\n",
    "                        X_test_samples = [self.X_test[k] for k in sample_idxs]\n",
    "                        Y_test_samples = [self.Y_test[k] for k in sample_idxs]\n",
    "                        avg_test_loss, avg_test_accuracy = self.__get_mean_evaluation(X_test_samples, \n",
    "                                                                                      Y_test_samples)\n",
    "                        print \"Validation: Loss = %.6f | Accuracy = %.6f\" % (avg_loss, avg_accuracy)\n",
    "                        print \"Test: Loss = %.6f | Accuracy = %.6f\" % (avg_test_loss, avg_test_accuracy)\n",
    "                        if avg_test_loss<self.best_loss and \\\n",
    "                           avg_test_accuracy>self.best_accuracy:\n",
    "                            self.best_loss = avg_test_loss\n",
    "                            self.best_accuracy = avg_test_accuracy\n",
    "                            self.__save_best_model(mode='best_test_model')\n",
    "                    if patience < current_iter:\n",
    "                        print \"TRAINING COMPLETE (at iteration %d)\" % current_iter\n",
    "                        return\n",
    "                \n",
    "    def predict(self, x): \n",
    "        '''\n",
    "        # x: sentences in string (e.g. \"i'd like to book a flight from london to paris\")\n",
    "        # return: a list of predicted labels associated with the words in the sentence x.\n",
    "        '''\n",
    "        x_filtered = filter(lambda w:w in self.dicts['words2idx'].keys(), x.split())\n",
    "        x_encoded = np.asarray([map(self.dicts['words2idx'].get,x_filtered)])\n",
    "        pred_encoded = map(np.argmax,self.model.predict_on_batch(x_encoded)[0])\n",
    "        return map(self.dicts['idx2labels'].get, pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/ojo_ner/ojo_ner/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT DATASET'S META-LABEL: arrive_time\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.428441 | Accuracy = 0.995222\n",
      "Test: Loss = 0.431019 | Accuracy = 0.994464\n",
      "... Epoch: 2\n",
      "TRAINING COMPLETE (at iteration 3983)\n",
      "CURRENT DATASET'S META-LABEL: depart_time_meal\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 1.505589 | Accuracy = 0.966046\n",
      "Test: Loss = 1.486618 | Accuracy = 0.979804\n",
      "New best_test_model Saved!\n",
      "... Epoch: 2\n",
      "TRAINING COMPLETE (at iteration 3983)\n",
      "CURRENT DATASET'S META-LABEL: return_cost\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.704529 | Accuracy = 0.996136\n",
      "Test: Loss = 0.705529 | Accuracy = 0.996758\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.025847 | Accuracy = 0.996867\n",
      "Test: Loss = 0.040120 | Accuracy = 0.994011\n",
      "... Epoch: 2\n",
      "TRAINING COMPLETE (at iteration 3983)\n",
      "CURRENT DATASET'S META-LABEL: flight\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 1.015349 | Accuracy = 0.971991\n",
      "Test: Loss = 1.004656 | Accuracy = 0.979812\n",
      "New best_test_model Saved!\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.131216 | Accuracy = 0.982539\n",
      "Test: Loss = 0.189206 | Accuracy = 0.969785\n",
      "... Epoch: 2\n",
      "TRAINING COMPLETE (at iteration 3983)\n",
      "CURRENT DATASET'S META-LABEL: from_loc\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 1.147208 | Accuracy = 0.881577\n",
      "Test: Loss = 1.205068 | Accuracy = 0.851426\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.430274 | Accuracy = 0.899132\n",
      "Test: Loss = 0.553417 | Accuracy = 0.865916\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.196168 | Accuracy = 0.948107\n",
      "Test: Loss = 0.406665 | Accuracy = 0.908816\n",
      "... Epoch: 2\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.156891 | Accuracy = 0.959691\n",
      "Test: Loss = 0.282392 | Accuracy = 0.940754\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.125817 | Accuracy = 0.968108\n",
      "Test: Loss = 0.224814 | Accuracy = 0.949415\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.089150 | Accuracy = 0.979410\n",
      "Test: Loss = 0.323350 | Accuracy = 0.945852\n",
      "... Epoch: 3\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.087591 | Accuracy = 0.979600\n",
      "Test: Loss = 0.149790 | Accuracy = 0.958595\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.054000 | Accuracy = 0.984816\n",
      "Test: Loss = 0.239040 | Accuracy = 0.955857\n",
      "... Epoch: 4\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.041524 | Accuracy = 0.988329\n",
      "Test: Loss = 0.128094 | Accuracy = 0.968385\n",
      "... Epoch: 5\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.037442 | Accuracy = 0.992082\n",
      "Test: Loss = 0.117017 | Accuracy = 0.966920\n",
      "... Epoch: 6\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.031183 | Accuracy = 0.993707\n",
      "Test: Loss = 0.095081 | Accuracy = 0.969813\n",
      "... Epoch: 7\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.025983 | Accuracy = 0.996287\n",
      "Test: Loss = 0.089072 | Accuracy = 0.972973\n",
      "... Epoch: 8\n",
      "... Epoch: 9\n",
      "... Epoch: 10\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.022983 | Accuracy = 0.996912\n",
      "Test: Loss = 0.069131 | Accuracy = 0.986097\n",
      "... Epoch: 11\n",
      "... Epoch: 12\n",
      "... Epoch: 13\n",
      "... Epoch: 14\n",
      "... Epoch: 15\n",
      "... Epoch: 16\n",
      "... Epoch: 17\n",
      "... Epoch: 18\n",
      "... Epoch: 19\n",
      "... Epoch: 20\n",
      "TRAINING COMPLETE (at iteration 78677)\n",
      "CURRENT DATASET'S META-LABEL: codes_types\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.993698 | Accuracy = 0.971230\n",
      "Test: Loss = 0.987555 | Accuracy = 0.973926\n",
      "New best_test_model Saved!\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.162068 | Accuracy = 0.977044\n",
      "Test: Loss = 0.185550 | Accuracy = 0.973461\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.082974 | Accuracy = 0.989159\n",
      "Test: Loss = 0.270889 | Accuracy = 0.958420\n",
      "... Epoch: 2\n",
      "TRAINING COMPLETE (at iteration 4983)\n",
      "CURRENT DATASET'S META-LABEL: misc_date_time\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.875108 | Accuracy = 1.000000\n",
      "Test: Loss = 0.882527 | Accuracy = 0.999474\n",
      "... Epoch: 2\n",
      "TRAINING COMPLETE (at iteration 3983)\n",
      "CURRENT DATASET'S META-LABEL: depart_date_mod_or\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.717308 | Accuracy = 0.962761\n",
      "Test: Loss = 0.725641 | Accuracy = 0.958294\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.218815 | Accuracy = 0.965148\n",
      "Test: Loss = 0.224316 | Accuracy = 0.958930\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.183270 | Accuracy = 0.967679\n",
      "Test: Loss = 0.212106 | Accuracy = 0.956642\n",
      "... Epoch: 2\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.109738 | Accuracy = 0.972884\n",
      "Test: Loss = 0.072108 | Accuracy = 0.982369\n",
      "New best_test_model Saved!\n",
      "... Epoch: 3\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.065172 | Accuracy = 0.984658\n",
      "Test: Loss = 0.043455 | Accuracy = 0.992045\n",
      "New best_test_model Saved!\n",
      "... Epoch: 4\n",
      "... Epoch: 5\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.022001 | Accuracy = 0.993881\n",
      "Test: Loss = 0.018973 | Accuracy = 0.994975\n",
      "New best_test_model Saved!\n",
      "... Epoch: 6\n",
      "... Epoch: 7\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.015852 | Accuracy = 0.996093\n",
      "Test: Loss = 0.017188 | Accuracy = 0.995563\n",
      "... Epoch: 8\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.012957 | Accuracy = 0.997093\n",
      "Test: Loss = 0.014492 | Accuracy = 0.997153\n",
      "... Epoch: 9\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.012621 | Accuracy = 0.997385\n",
      "Test: Loss = 0.011092 | Accuracy = 0.997153\n",
      "... Epoch: 10\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.009509 | Accuracy = 0.998973\n",
      "Test: Loss = 0.012432 | Accuracy = 0.997153\n",
      "... Epoch: 11\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.007263 | Accuracy = 0.999286\n",
      "Test: Loss = 0.013280 | Accuracy = 0.997153\n",
      "... Epoch: 12\n",
      "... Epoch: 13\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.006323 | Accuracy = 1.000000\n",
      "Test: Loss = 0.015156 | Accuracy = 0.995134\n",
      "... Epoch: 14\n",
      "... Epoch: 15\n",
      "... Epoch: 16\n",
      "... Epoch: 17\n",
      "... Epoch: 18\n",
      "... Epoch: 19\n",
      "... Epoch: 20\n",
      "CURRENT DATASET'S META-LABEL: stop_to_loc\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.807234 | Accuracy = 0.904554\n",
      "Test: Loss = 0.837024 | Accuracy = 0.888216\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.297016 | Accuracy = 0.947567\n",
      "Test: Loss = 0.350258 | Accuracy = 0.889694\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.176724 | Accuracy = 0.957941\n",
      "Test: Loss = 0.159234 | Accuracy = 0.962543\n",
      "New best_test_model Saved!\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.105738 | Accuracy = 0.976561\n",
      "Test: Loss = 0.104218 | Accuracy = 0.969155\n",
      "... Epoch: 2\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.043699 | Accuracy = 0.990506\n",
      "Test: Loss = 0.081309 | Accuracy = 0.984576\n",
      "... Epoch: 3\n",
      "... Epoch: 4\n",
      "TRAINING COMPLETE (at iteration 12949)\n",
      "CURRENT DATASET'S META-LABEL: arrive_date\n",
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.265629 | Accuracy = 0.996905\n",
      "Test: Loss = 0.267424 | Accuracy = 0.997413\n",
      "New best_valid_model Saved!\n",
      "Validation: Loss = 0.022819 | Accuracy = 0.997284\n",
      "Test: Loss = 0.002105 | Accuracy = 1.000000\n",
      "New best_test_model Saved!\n",
      "... Epoch: 2\n",
      "TRAINING COMPLETE (at iteration 3983)\n"
     ]
    }
   ],
   "source": [
    "performances = defaultdict(dict)\n",
    "for meta_label in meta_labels:\n",
    "    print \"CURRENT DATASET'S META-LABEL: %s\" % meta_label\n",
    "    performance = defaultdict()\n",
    "    dataset = datasets[meta_label]\n",
    "    ner = NER(dataset,model_dir=model_dir)\n",
    "    ner.train(validation=1,validation_freq=1000,verbose=1,nepochs=20)\n",
    "    performance['best_loss'] = ner.best_loss\n",
    "    performance['best_accuracy'] = ner.best_accuracy\n",
    "    performances[meta_label] = performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_to_loc: best_loss: 0.043699 | best_accuracy: 0.990506\n",
      "flight: best_loss: 0.131216 | best_accuracy: 0.982539\n",
      "codes_types: best_loss: 0.082974 | best_accuracy: 0.989159\n",
      "depart_date_mod_or: best_loss: 0.006323 | best_accuracy: 1.000000\n",
      "misc_date_time: best_loss: 0.875108 | best_accuracy: 1.000000\n",
      "arrive_time: best_loss: 0.428441 | best_accuracy: 0.995222\n",
      "arrive_date: best_loss: 0.002105 | best_accuracy: 1.000000\n",
      "depart_time_meal: best_loss: 1.486618 | best_accuracy: 0.979804\n",
      "return_cost: best_loss: 0.025847 | best_accuracy: 0.996867\n",
      "from_loc: best_loss: 0.022983 | best_accuracy: 0.996912\n"
     ]
    }
   ],
   "source": [
    "for meta_label,performance in performances.iteritems():\n",
    "    print \"%s: best_loss: %.6f | best_accuracy: %.6f\" % (meta_label,performance['best_loss'],performance['best_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Model Ensemble on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
