{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip, pickle, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Activation, TimeDistributed\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_atis(path, folder=0):\n",
    "    \n",
    "    folders = {0: 'atis.fold0.pkl.gz',\n",
    "               1: 'atis.fold1.pkl.gz',\n",
    "               2: 'atis.fold2.pkl.gz',\n",
    "               3: 'atis.fold3.pkl.gz',\n",
    "               4: 'atis.fold4.pkl.gz'}\n",
    "    \n",
    "    f = gzip.open(path+folders[folder], 'rb')\n",
    "    train, valid, test, dicts = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    train = (train[0],train[2])\n",
    "    valid = (valid[0],valid[2])\n",
    "    test = (test[0],test[2])\n",
    "\n",
    "    return {'train':train, 'valid':valid, 'test':test, 'dicts':dicts}\n",
    "\n",
    "# ATIS PATH: /Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/LUIS/DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NER:\n",
    "    \n",
    "    def __init__(self, data, model_dir, mode='simple_rnn'):\n",
    "        '''\n",
    "        # data: a dictionary which contains ...\n",
    "            {'train':train, 'valid':valid, 'test':test, 'dicts':dicts}\n",
    "            each value in the dictionary is a 2-tuple ...\n",
    "            (encoded_sentences, encoded_labels)\n",
    "            dicts includes ...\n",
    "            'words2idx', 'labels2idx'\n",
    "        # mode: 'simple_rnn', 'lstm', 'gru'.\n",
    "        '''\n",
    "        if mode not in ['simple_rnn','lstm','gru']:\n",
    "            print \"MODE ERROR: only 'simple_rnn','lstm','gru'. \\n\"\n",
    "            return\n",
    "        self.mode = mode\n",
    "        self.model_dir = model_dir # NB: this is for temporary model saving pathing.\n",
    "        \n",
    "        transform = {'x': lambda x: np.asarray([x]), \n",
    "                     'y': lambda y: to_categorical(np.asarray(y)[:,np.newaxis],\n",
    "                                                   len(data['dicts']['labels2idx']))[np.newaxis,:,:]}\n",
    "            # len(data['dicts']['labels2idx']): number of labels.\n",
    "        try:\n",
    "            data_transformed = [ (map(transform['x'],x), map(transform['y'],y)) \n",
    "                                 for x,y in [data['train'], data['valid'], data['test']] ]\n",
    "            self.X_train, self.Y_train = data_transformed[0]\n",
    "            self.X_valid, self.Y_valid = data_transformed[1]\n",
    "            self.X_test, self.Y_test = data_transformed[2]\n",
    "            self.dicts = data['dicts']\n",
    "        except:\n",
    "            print \"DATA FORMAT ERROR: \\n\", \\\n",
    "                  \"data = {'train':train, 'valid':valid, 'test':test, 'dicts':dicts} \\n\", \\\n",
    "                  \"value = (encoded_sentences, encoded_labels) \\n\"\n",
    "    \n",
    "    def __shuffle(self, X, Y, seed):\n",
    "        '''\n",
    "        # X, Y: data and corresponding labels.\n",
    "        # seed: ensure the same after-shuffle order for X and Y.\n",
    "        '''\n",
    "        random.seed(seed)\n",
    "        random.shuffle(X)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(Y)\n",
    "    \n",
    "    def __get_mean_evaluation(self, X, Y, model):\n",
    "        '''\n",
    "        # X, Y: data and corresponding labels.\n",
    "        # return: average loss and accuracy on X and Y\n",
    "        '''\n",
    "        evaluation_size = len(X)\n",
    "        losses, accuracies = [], []\n",
    "        for i in xrange(evaluation_size):\n",
    "            loss,accuracy = model.evaluate(X[i],Y[i],verbose=0)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "        return (np.mean(losses),np.mean(accuracies))\n",
    "\n",
    "    def __save_best_model(self):\n",
    "        '''\n",
    "        # save current model as the best when called.\n",
    "        '''\n",
    "        try:\n",
    "            os.remove(self.model_dir+'best_model.json')\n",
    "            os.remove(self.model_dir+'best_weights.h5')\n",
    "        except OSError:\n",
    "            pass\n",
    "        model_json = self.model.to_json()\n",
    "        open(self.model_dir+'best_model.json','w').write(model_json)\n",
    "        self.model.save_weights(self.model_dir+'best_weights.h5')\n",
    "        print \"New Best Model Saved!\"\n",
    "    \n",
    "    def train(self, validation=False, validation_freq=1, verbose=0, verbose_freq=100,\n",
    "                    lr=.1, nhidden=100, emb_dim=100, nepochs=1,\n",
    "                    regularize=False, reg_method='l2',lmd=.1):\n",
    "        '''\n",
    "        # validation, validation_freq: \n",
    "            if true, run validation at validation_freq epoch (1 by default).\n",
    "        # verbose, verbose_freq: \n",
    "            0: only print out simple messages (e.g. '... building models').\n",
    "            1: print out validation too.\n",
    "            print training progress after training every verbose_freq sentences.\n",
    "        # lr: learning rate.\n",
    "        # nhidden: number of hidden neurons.\n",
    "        # emb_dim: dimension of word embeddings.\n",
    "        # nepochs: number of epochs.\n",
    "        # regularize: regularize or not.\n",
    "        # reg_method: Lasso or L2 regularization.\n",
    "        # lmd: regularization hyperparam.\n",
    "        # return: end training when max patience is reached.\n",
    "        '''\n",
    "        print \"... configuring model\"\n",
    "        vocsize = len(self.dicts['words2idx'])\n",
    "        nlabels = len(self.dicts['labels2idx'])\n",
    "        nsents = len(self.X_train)\n",
    "        self.dicts['idx2labels'] = {i:l for l,i in self.dicts['labels2idx'].iteritems()}\n",
    "        self.dicts['idx2words'] = {i:w for w,i in self.dicts['words2idx'].iteritems()}\n",
    "            \n",
    "        print \"... building model\" \n",
    "        if regularize:\n",
    "            regularizer = l2(lmd) if reg_method=='l2' else l1(lmd)\n",
    "        else:\n",
    "            regularizer = None\n",
    "        model_types = {'simple_rnn':SimpleRNN, 'lstm':LSTM, 'gru':GRU}\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim=vocsize, output_dim=emb_dim))\n",
    "        self.model.add(model_types[self.mode](output_dim=nhidden, activation='sigmoid', \n",
    "                       return_sequences=True, W_regularizer=regularizer))\n",
    "        self.model.add(TimeDistributed(Dense(output_dim=nlabels)))\n",
    "        self.model.add(Activation('softmax'))\n",
    "        sgd = SGD(lr=lr, momentum=.0, decay=.0, nesterov=False)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        \n",
    "        self.best_model = self.model # initial model, updated when new best models are saved.\n",
    "        \n",
    "        print \"... training model\"\n",
    "        patience = 3000\n",
    "        patience_increase_ratio = 2\n",
    "        improvement_threshold = .995\n",
    "        best_iter = 0\n",
    "        best_loss = np.inf\n",
    "        best_accuracy = 0.\n",
    "        done_looping = False # stop training when patience are broken.\n",
    "        num_iter = 0\n",
    "        for e in xrange(nepochs):\n",
    "            if done_looping: break\n",
    "            e += 1\n",
    "            if verbose: print \"... Epoch: %d\" % e\n",
    "            self.__shuffle(self.X_train,self.Y_train,seed=0)\n",
    "            for i in xrange(nsents):\n",
    "                num_iter += 1\n",
    "                if verbose and i!=0 and i%verbose_freq==0:\n",
    "                    print \"    ... trained %d sentences\" % i\n",
    "                if self.X_train[i].shape[1]==1: continue\n",
    "                self.model.train_on_batch(self.X_train[i],self.Y_train[i])\n",
    "        \n",
    "                if validation and i%validation_freq==0:\n",
    "                    sample_idxs = random.sample(range(len(self.X_valid)),100)\n",
    "                    X_valid_samples = [self.X_valid[k] for k in sample_idxs]\n",
    "                    Y_valid_samples = [self.Y_valid[k] for k in sample_idxs]\n",
    "                    avg_loss, avg_accuracy = self.__get_mean_evaluation(X_valid_samples, \n",
    "                                                                        Y_valid_samples,\n",
    "                                                                        self.model)\n",
    "                    if avg_loss < best_loss*improvement_threshold and \\\n",
    "                       avg_accuracy > best_accuracy:\n",
    "                        self.__save_best_model()\n",
    "                        patience = max(patience, num_iter*patience_increase_ratio)\n",
    "                        best_loss = avg_loss\n",
    "                        best_accuracy = avg_accuracy\n",
    "                        best_iter = num_iter\n",
    "                        print \"Validation: Loss = %.6f | Accuracy = %.6f\" % (avg_loss, avg_accuracy)                        \n",
    "                    if patience <= num_iter:\n",
    "                        done_looping = True\n",
    "                        break\n",
    "\n",
    "        mean_test_loss, mean_test_accuracy = self.__get_mean_evaluation(self.X_test,\n",
    "                                                                        self.Y_test,\n",
    "                                                                        self.best_model)\n",
    "        print \"TRAINING COMPLETE (at iteration %d)!\" % num_iter\n",
    "        print \"Test on Best Model: Loss = %.6f | Accuracy = %.6f\" % (mean_test_loss,mean_test_accuracy)\n",
    "                \n",
    "    def predict(self, x): \n",
    "        '''\n",
    "        # x: sentences in string (e.g. \"i'd like to book a flight from london to paris\")\n",
    "        # return: a list of predicted labels associated with the words in the sentence x.\n",
    "        '''\n",
    "        x_filtered = filter(lambda w:w in self.dicts['words2idx'].keys(), x.split())\n",
    "        x_encoded = np.asarray([map(self.dicts['words2idx'].get,x_filtered)])\n",
    "        pred_encoded = map(np.argmax,self.model.predict_on_batch(x_encoded)[0])\n",
    "        return map(self.dicts['idx2labels'].get, pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = load_atis(path='/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/LUIS/DATA/')\n",
    "model_dir = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/ojo_ner/ojo_ner/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ner = NER(data,model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 3.151388 | Accuracy = 0.646422\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 1.500056 | Accuracy = 0.662491\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.876027 | Accuracy = 0.816586\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 2\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.804577 | Accuracy = 0.829873\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.620858 | Accuracy = 0.864586\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 3\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.564171 | Accuracy = 0.880401\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.484174 | Accuracy = 0.895482\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.420116 | Accuracy = 0.912161\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 4\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.364694 | Accuracy = 0.927968\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 5\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.317037 | Accuracy = 0.933053\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.263734 | Accuracy = 0.937477\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 6\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.204444 | Accuracy = 0.952956\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 7\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.172319 | Accuracy = 0.960388\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 8\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 9\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 10\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.117927 | Accuracy = 0.966539\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 11\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.114147 | Accuracy = 0.974256\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 12\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 13\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 14\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.102393 | Accuracy = 0.974323\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 15\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 16\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 17\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 18\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.087298 | Accuracy = 0.977847\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 19\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "... Epoch: 20\n",
      "    ... trained 500 sentences\n",
      "    ... trained 1000 sentences\n",
      "    ... trained 1500 sentences\n",
      "    ... trained 2000 sentences\n",
      "    ... trained 2500 sentences\n",
      "    ... trained 3000 sentences\n",
      "    ... trained 3500 sentences\n",
      "TRAINING COMPLETE (at iteration 79660)!\n",
      "Test on Best Model: Loss = 0.217458 | Accuracy = 0.956015\n"
     ]
    }
   ],
   "source": [
    "ner.train(validation=1,validation_freq=1000,verbose=1,verbose_freq=500,\n",
    "          nhidden=300,nepochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n"
     ]
    }
   ],
   "source": [
    "sent = \"i'd like to book a flight from charlotte to las vegas\"\n",
    "print ner.predict(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM & GRU TESTS (W/O REGULARIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 2.352198 | Accuracy = 0.646422\n",
      "... Epoch: 2\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.344126 | Accuracy = 0.919698\n",
      "... Epoch: 3\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.195892 | Accuracy = 0.955229\n",
      "... Epoch: 4\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.168471 | Accuracy = 0.966054\n",
      "... Epoch: 5\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.121325 | Accuracy = 0.976052\n",
      "... Epoch: 6\n",
      "... Epoch: 7\n",
      "... Epoch: 8\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.105804 | Accuracy = 0.976203\n",
      "... Epoch: 9\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.104694 | Accuracy = 0.976819\n",
      "... Epoch: 10\n",
      "... Epoch: 11\n",
      "... Epoch: 12\n",
      "... Epoch: 13\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.091405 | Accuracy = 0.984327\n",
      "... Epoch: 14\n",
      "... Epoch: 15\n",
      "... Epoch: 16\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.090592 | Accuracy = 0.984757\n",
      "... Epoch: 17\n",
      "... Epoch: 18\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.088189 | Accuracy = 0.985312\n",
      "... Epoch: 19\n",
      "... Epoch: 20\n",
      "... Epoch: 21\n",
      "... Epoch: 22\n",
      "... Epoch: 23\n",
      "... Epoch: 24\n",
      "... Epoch: 25\n",
      "... Epoch: 26\n",
      "... Epoch: 27\n",
      "... Epoch: 28\n",
      "... Epoch: 29\n",
      "... Epoch: 30\n",
      "... Epoch: 31\n",
      "... Epoch: 32\n",
      "... Epoch: 33\n",
      "... Epoch: 34\n",
      "... Epoch: 35\n",
      "... Epoch: 36\n",
      "TRAINING COMPLETE (at iteration 139406)!\n",
      "Test on Best Model: Loss = 0.231208 | Accuracy = 0.967235\n",
      "CPU times: user 7min 15s, sys: 2.47 s, total: 7min 17s\n",
      "Wall time: 7min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dir = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/ojo_ner/ojo_ner/models/LSTM/'\n",
    "ner_LSTM = NER(data,model_dir=model_dir,mode='lstm')\n",
    "ner_LSTM.train(validation=1,validation_freq=10000,verbose=1,verbose_freq=5000,lr=1.,nhidden=100,nepochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... configuring model\n",
      "... building model\n",
      "... training model\n",
      "... Epoch: 1\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 2.498494 | Accuracy = 0.646422\n",
      "... Epoch: 2\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.314833 | Accuracy = 0.937377\n",
      "... Epoch: 3\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.179257 | Accuracy = 0.957094\n",
      "... Epoch: 4\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.163287 | Accuracy = 0.966786\n",
      "... Epoch: 5\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.121273 | Accuracy = 0.973163\n",
      "... Epoch: 6\n",
      "... Epoch: 7\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.116239 | Accuracy = 0.978026\n",
      "... Epoch: 8\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.094448 | Accuracy = 0.981240\n",
      "... Epoch: 9\n",
      "... Epoch: 10\n",
      "... Epoch: 11\n",
      "... Epoch: 12\n",
      "... Epoch: 13\n",
      "New Best Model Saved!\n",
      "Validation: Loss = 0.085655 | Accuracy = 0.985407\n",
      "... Epoch: 14\n",
      "... Epoch: 15\n",
      "... Epoch: 16\n",
      "... Epoch: 17\n",
      "... Epoch: 18\n",
      "... Epoch: 19\n",
      "... Epoch: 20\n",
      "... Epoch: 21\n",
      "... Epoch: 22\n",
      "... Epoch: 23\n",
      "... Epoch: 24\n",
      "... Epoch: 25\n",
      "... Epoch: 26\n",
      "TRAINING COMPLETE (at iteration 99576)!\n",
      "Test on Best Model: Loss = 0.232135 | Accuracy = 0.967484\n",
      "CPU times: user 4min 3s, sys: 954 ms, total: 4min 4s\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dir = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/OJO/ojo_ner/ojo_ner/models/GRU/'\n",
    "ner_GRU = NER(data,model_dir=model_dir,mode='gru')\n",
    "ner_GRU.train(validation=1,validation_freq=10000,verbose=1,verbose_freq=5000,lr=1.,nhidden=100,nepochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
